{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 9873724,
          "sourceType": "datasetVersion",
          "datasetId": 6018955
        }
      ],
      "dockerImageVersionId": 30786,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Mistral 7B finetuned on Medical Dialogues"
      ],
      "metadata": {
        "id": "JdXiJzMm7ewB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By Lotte Michels"
      ],
      "metadata": {
        "id": "mSHLDknqdzgJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing and Importing Libraries"
      ],
      "metadata": {
        "id": "yCIWKTwPdzgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch transformers datasets accelerate peft trl\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T09:26:55.511021Z",
          "iopub.execute_input": "2024-11-22T09:26:55.511306Z",
          "iopub.status.idle": "2024-11-22T09:27:15.184632Z",
          "shell.execute_reply.started": "2024-11-22T09:26:55.511273Z",
          "shell.execute_reply": "2024-11-22T09:27:15.183659Z"
        },
        "id": "HSyZp3AEdzgK",
        "outputId": "a0a42f73-01fb-4333-8393-6308de0e25d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.34.2)\nCollecting peft\n  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\nCollecting trl\n  Downloading trl-0.12.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from trl) (13.7.1)\nCollecting transformers\n  Downloading transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->trl) (2.18.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nDownloading peft-0.13.2-py3-none-any.whl (320 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading trl-0.12.1-py3-none-any.whl (310 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.9/310.9 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading transformers-4.46.3-py3-none-any.whl (10.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m108.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: transformers, trl, peft\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.45.1\n    Uninstalling transformers-4.45.1:\n      Successfully uninstalled transformers-4.45.1\nSuccessfully installed peft-0.13.2 transformers-4.46.3 trl-0.12.1\nNote: you may need to restart the kernel to use updated packages.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U bitsandbytes"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T09:27:15.186192Z",
          "iopub.execute_input": "2024-11-22T09:27:15.186479Z",
          "iopub.status.idle": "2024-11-22T09:27:27.177898Z",
          "shell.execute_reply.started": "2024-11-22T09:27:15.186452Z",
          "shell.execute_reply": "2024-11-22T09:27:27.176841Z"
        },
        "id": "hLwTt87IdzgM",
        "outputId": "54420caa-85c2-4e89-df81-f60a25b52473"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting bitsandbytes\n  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.44.1\nNote: you may need to restart the kernel to use updated packages.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U \"huggingface_hub[cli]\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T09:27:27.179361Z",
          "iopub.execute_input": "2024-11-22T09:27:27.179769Z",
          "iopub.status.idle": "2024-11-22T09:27:36.367110Z",
          "shell.execute_reply.started": "2024-11-22T09:27:27.179710Z",
          "shell.execute_reply": "2024-11-22T09:27:36.366055Z"
        },
        "id": "tw_kanEmdzgM",
        "outputId": "d5ff982a-06b8-455e-9958-96206b719893"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: huggingface_hub[cli] in /opt/conda/lib/python3.10/site-packages (0.25.1)\nCollecting huggingface_hub[cli]\n  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub[cli]) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub[cli]) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub[cli]) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub[cli]) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub[cli]) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub[cli]) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub[cli]) (4.12.2)\nCollecting InquirerPy==0.3.4 (from huggingface_hub[cli])\n  Downloading InquirerPy-0.3.4-py3-none-any.whl.metadata (8.1 kB)\nCollecting pfzy<0.4.0,>=0.3.1 (from InquirerPy==0.3.4->huggingface_hub[cli])\n  Downloading pfzy-0.3.4-py3-none-any.whl.metadata (4.9 kB)\nRequirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (3.0.47)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub[cli]) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub[cli]) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub[cli]) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub[cli]) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub[cli]) (2024.8.30)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface_hub[cli]) (0.2.13)\nDownloading InquirerPy-0.3.4-py3-none-any.whl (67 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pfzy-0.3.4-py3-none-any.whl (8.5 kB)\nInstalling collected packages: pfzy, InquirerPy, huggingface_hub\n  Attempting uninstall: huggingface_hub\n    Found existing installation: huggingface-hub 0.25.1\n    Uninstalling huggingface-hub-0.25.1:\n      Successfully uninstalled huggingface-hub-0.25.1\nSuccessfully installed InquirerPy-0.3.4 huggingface_hub-0.26.2 pfzy-0.3.4\nNote: you may need to restart the kernel to use updated packages.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tf-keras"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T09:27:36.369831Z",
          "iopub.execute_input": "2024-11-22T09:27:36.370175Z",
          "iopub.status.idle": "2024-11-22T09:27:44.637041Z",
          "shell.execute_reply.started": "2024-11-22T09:27:36.370147Z",
          "shell.execute_reply": "2024-11-22T09:27:44.635278Z"
        },
        "id": "ZEnF6h9pdzgN",
        "outputId": "e07b2979-747d-4b9c-99e1-0e6a23b26da2"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: tf-keras in /opt/conda/lib/python3.10/site-packages (2.16.0)\nRequirement already satisfied: tensorflow<2.17,>=2.16 in /opt/conda/lib/python3.10/site-packages (from tf-keras) (2.16.1)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (24.3.25)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (0.5.4)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (0.2.0)\nRequirement already satisfied: h5py>=3.10.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (3.11.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (18.1.1)\nRequirement already satisfied: ml-dtypes~=0.3.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (0.3.2)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (2.32.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (70.0.0)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.16.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (4.12.2)\nRequirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.16.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.62.2)\nRequirement already satisfied: tensorboard<2.17,>=2.16 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (2.16.2)\nRequirement already satisfied: keras>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (3.3.3)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (0.37.0)\nRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.26.4)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow<2.17,>=2.16->tf-keras) (0.43.0)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (13.7.1)\nRequirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (0.0.8)\nRequirement already satisfied: optree in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (0.11.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (2024.8.30)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras) (3.6)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras) (3.0.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow<2.17,>=2.16->tf-keras) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras) (2.1.5)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (2.18.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (0.1.2)\nNote: you may need to restart the kernel to use updated packages.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wandb"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T09:27:44.638844Z",
          "iopub.execute_input": "2024-11-22T09:27:44.639726Z",
          "iopub.status.idle": "2024-11-22T09:27:52.652593Z",
          "shell.execute_reply.started": "2024-11-22T09:27:44.639680Z",
          "shell.execute_reply": "2024-11-22T09:27:52.651555Z"
        },
        "id": "iH8ZC2-FdzgN",
        "outputId": "5ca87975-95a2-49b3-91b9-a3bc51e2a200"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.18.3)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.43)\nRequirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb) (3.11.0)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.32.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.15.0)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (70.0.0)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\nNote: you may need to restart the kernel to use updated packages.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments, pipeline, logging\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
        "import os, torch, wandb\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T09:27:52.653864Z",
          "iopub.execute_input": "2024-11-22T09:27:52.654144Z",
          "iopub.status.idle": "2024-11-22T09:28:13.320404Z",
          "shell.execute_reply.started": "2024-11-22T09:27:52.654116Z",
          "shell.execute_reply": "2024-11-22T09:28:13.319700Z"
        },
        "id": "Gdq93DzodzgO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Med-Dialog Dataset"
      ],
      "metadata": {
        "id": "e_s1QiOU98Rc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data contains two .txt files per data split (train, validation, test). A source.txt file contains the patient queries, a target.txt file contains the doctor answers. Examples are displayed in the cell below."
      ],
      "metadata": {
        "id": "aVTGNCWOdzgO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display query example\n",
        "with open(\"/kaggle/input/dialog-with-term/clean.val.source.txt\", \"r\") as sourcefile:\n",
        "    firstquery = sourcefile.readline()\n",
        "    print(firstquery)\n",
        "\n",
        "# Display corresponding response example\n",
        "with open(\"/kaggle/input/dialog-with-term/clean.val.target.txt\", \"r\") as targetfile:\n",
        "    firstreply = targetfile.readline()\n",
        "    print(firstreply)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T09:51:11.519225Z",
          "iopub.execute_input": "2024-11-22T09:51:11.519946Z",
          "iopub.status.idle": "2024-11-22T09:51:11.555251Z",
          "shell.execute_reply.started": "2024-11-22T09:51:11.519913Z",
          "shell.execute_reply": "2024-11-22T09:51:11.554495Z"
        },
        "id": "NLSkl7t4dzgO",
        "outputId": "94d1a2af-6d7f-4c0d-f4b1-13d6221c2b61"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "patient: hi, i have an 8 year old son, he just told me about a little knot probably as big as my fingertip underneath his skin, in the area right below his armpit but not far down his side. kind of feels like a little hard bubble that moves around a pit when you touch it.\n\nhi,from history it seems that there might be having enlarged axillary lymph node giving this problem.he might have some cut or minor injury in his arm and having some minor infection causing this node enlarged.if there is infection on hand go for its treatment after consulting your doctor.ok and take care.\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each split, the source and corresponding target files are combined into a .json file that juxtaposes input and output for each sample.  "
      ],
      "metadata": {
        "id": "7yy2bvntdzgP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve dataset(s) of interest\n",
        "splits = ['val'] # add 'train' and 'test' if you want to include these splits\n",
        "\n",
        "for split in splits:\n",
        "    # Combine source and target files into a .json format\n",
        "    with open(\"/kaggle/input/dialog-with-term/clean.{}.source.txt\".format(split), \"r\") as source_file, open(\"/kaggle/input/dialog-with-term/clean.{}.target.txt\".format(split), \"r\") as target_file:\n",
        "        # Read all lines from both files\n",
        "        source_lines = source_file.readlines()\n",
        "        print('{} set contains {} samples'.format(split, len(source_lines)))\n",
        "        target_lines = target_file.readlines()\n",
        "\n",
        "        # Ensure both files have the same number of lines\n",
        "        if len(source_lines) != len(target_lines):\n",
        "            raise ValueError(\"The number of lines in the source and target files do not match.\")\n",
        "\n",
        "        # Create a list of dictionaries for each dialog pair\n",
        "        data = []\n",
        "        for source, target in zip(source_lines, target_lines):\n",
        "            source = source.strip() # Strip any leading/trailing whitespace (including newline characters)\n",
        "            source = source.replace('patient: ', '') # Remove the 'patient: ' promt from the string (additional data cleaning)\n",
        "            target = target.strip() # Strip any leading/trailing whitespace (including newline characters)\n",
        "\n",
        "            # Format sample\n",
        "            sample = f\"<|startofquestion|> {source} <|endofquestion|> <|startofanswer|> {target} <|endofanswer|>\"\n",
        "\n",
        "            # Append it to the data list\n",
        "            data.append({\"text\": sample})\n",
        "\n",
        "    # Write the data to a .json file\n",
        "    with open(\"/kaggle/working/clean_{}_dialog_data.json\".format(split), \"w\") as json_file:\n",
        "        json.dump(data, json_file, indent=4)\n",
        "\n",
        "    print(\"{} data has been converted to JSON format\".format(split))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T09:51:14.407437Z",
          "iopub.execute_input": "2024-11-22T09:51:14.408241Z",
          "iopub.status.idle": "2024-11-22T09:51:14.749688Z",
          "shell.execute_reply.started": "2024-11-22T09:51:14.408205Z",
          "shell.execute_reply": "2024-11-22T09:51:14.748859Z"
        },
        "id": "gdEhhOVldzgP",
        "outputId": "344fe05a-d889-4f5c-d92b-1acc732d674b"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "val set contains 12351 samples\nval data has been converted to JSON format\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "interm_dataset = load_dataset(\n",
        "    \"json\",\n",
        "    data_files={\n",
        "        #\"train\": \"/kaggle/working/train_dialog_data.json\",\n",
        "        \"validation\": \"/kaggle/working/clean_val_dialog_data.json\",\n",
        "        #\"test\": \"/kaggle/working/test_dialog_data.json\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# Select 15% of the loaded datasplit (for computational constraint reasons)\n",
        "subset = interm_dataset[\"validation\"].train_test_split(test_size=0.1)\n",
        "dataset = subset[\"test\"]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T09:51:27.912877Z",
          "iopub.execute_input": "2024-11-22T09:51:27.913693Z",
          "iopub.status.idle": "2024-11-22T09:51:28.575734Z",
          "shell.execute_reply.started": "2024-11-22T09:51:27.913658Z",
          "shell.execute_reply": "2024-11-22T09:51:28.574880Z"
        },
        "colab": {
          "referenced_widgets": [
            "d552b8eea2754cd6b063e3b546838ccb"
          ]
        },
        "id": "Z68zJVFUdzgQ",
        "outputId": "794fc09b-d024-4b24-dbb5-ce12bb78633d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating validation split: 0 examples [00:00, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d552b8eea2754cd6b063e3b546838ccb"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check a data sample:"
      ],
      "metadata": {
        "id": "Rtps82YRdzgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check a data sample\n",
        "print('total of {} samples'.format(len(dataset)))\n",
        "print(dataset[0])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T09:51:31.643898Z",
          "iopub.execute_input": "2024-11-22T09:51:31.644625Z",
          "iopub.status.idle": "2024-11-22T09:51:31.652744Z",
          "shell.execute_reply.started": "2024-11-22T09:51:31.644591Z",
          "shell.execute_reply": "2024-11-22T09:51:31.651959Z"
        },
        "id": "Rnzwvgc5dzgQ",
        "outputId": "78269c79-521c-4d18-f3de-2651e4287263"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "total of 1236 samples\n{'text': '<|startofquestion|> hi doctor, i have a problem with my mouth smell bad when talking with other.i always clean my teeth twice a day and usually go to see dentist clinic. they sad my teeth is no bacteria and clean. they comment me to see a doctor who specialist nose, or stomachcould you tell me how can i make fresh air <|endofquestion|> <|startofanswer|> hi! welcome to healthcaremagic!i read your query. bad breath can be due to local oral causes or as a result of systemic causes. if your mouth is clean, look for other symptoms. dry mouth for long durations, diabetes, high carbohydrate diet, gastrointestinal disease, lung disease, liver, kidney, blood, all can lead to bad breath.i suggest you to meet a physician and give history and get check up done. do you have any medical problem? any acid reflux or acidity? get lft, kft, chest xray, ent check up and blood test done. after confirmation of any underlying disease, it needs to be cured.besides that, maintain good oral hygiene. use mouthwash. drink lots of fluids and avoid dry mouth. chew sugarless gums. massage gums with gum paint.have healthy diet with multivitamins and minerals. follow up regularly for oral prophylaxis.at times even after cure of bad breath, patient has a feeling of bad breath. its psychogenic. rule this out with the help of dental check up.hope the answer helps you. thank you! <|endofanswer|>'}\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Large Language Model"
      ],
      "metadata": {
        "id": "0pWtuhOjN6UU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Model"
      ],
      "metadata": {
        "id": "ezJirXNkaeVC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The base model is loaded via Hugging Face and the Weights and Biases library is used to keep track of the training process:"
      ],
      "metadata": {
        "id": "aSE9xfsVdzgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Access API's vor Hugging Face (for model extraction) and WandB (for keeping track of the model performance during training)\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "user_secrets = UserSecretsClient()\n",
        "\n",
        "secret_hf = user_secrets.get_secret(\"huggingface\") # Access secret Hugging Face API key\n",
        "!huggingface-cli login --token $secret_hf\n",
        "\n",
        "secret_wandb = user_secrets.get_secret(\"wandb\") # Access secret Weights and Biases API key\n",
        "wandb.login(key = secret_wandb)\n",
        "run = wandb.init(\n",
        "    project='Finetuning Mistral 7B',\n",
        "    job_type=\"training\",\n",
        "    anonymous=\"allow\"\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-20T12:49:45.120922Z",
          "iopub.execute_input": "2024-11-20T12:49:45.121175Z",
          "iopub.status.idle": "2024-11-20T12:49:51.245192Z",
          "shell.execute_reply.started": "2024-11-20T12:49:45.121139Z",
          "shell.execute_reply": "2024-11-20T12:49:51.244444Z"
        },
        "colab": {
          "referenced_widgets": [
            "77b445e99b4a467e9e312f742f7cb5c3"
          ]
        },
        "id": "jMLu4jo3dzgR",
        "outputId": "5394a526-2f7b-4672-a6d9-ab3151731e43"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nThe token `meddaiproject` has been saved to /root/.cache/huggingface/stored_tokens\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful.\nThe current active token is: `meddaiproject`\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlotte-michels-nl\u001b[0m (\u001b[33mlotte-michels\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113798911110785, max=1.0…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "77b445e99b4a467e9e312f742f7cb5c3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.18.3"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20241120_124948-yp0zjww5</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/lotte-michels/Finetuning%20Mistral%207B/runs/yp0zjww5' target=\"_blank\">hardy-plasma-16</a></strong> to <a href='https://wandb.ai/lotte-michels/Finetuning%20Mistral%207B' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/lotte-michels/Finetuning%20Mistral%207B' target=\"_blank\">https://wandb.ai/lotte-michels/Finetuning%20Mistral%207B</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/lotte-michels/Finetuning%20Mistral%207B/runs/yp0zjww5' target=\"_blank\">https://wandb.ai/lotte-michels/Finetuning%20Mistral%207B/runs/yp0zjww5</a>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Load LLM and configuration function (for quicker training)\n",
        "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit= True,\n",
        "    bnb_4bit_quant_type= \"nf4\",\n",
        "    bnb_4bit_compute_dtype= torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant= False,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        quantization_config=bnb_config,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        ")\n",
        "\n",
        "model.config.pretraining_tp = 1\n",
        "model.gradient_checkpointing_enable()\n",
        "model.config.use_cache = False\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    r=64,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"]\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)"
      ],
      "metadata": {
        "id": "NLHr_hhG0DyG",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-20T12:49:51.246288Z",
          "iopub.execute_input": "2024-11-20T12:49:51.246562Z",
          "iopub.status.idle": "2024-11-20T12:51:40.528698Z",
          "shell.execute_reply.started": "2024-11-20T12:49:51.246534Z",
          "shell.execute_reply": "2024-11-20T12:51:40.527774Z"
        },
        "colab": {
          "referenced_widgets": [
            "a7e37f321fb245e798aa3153f43f31ee",
            "d5f91a236ffe4b5fa89779b4c04e3cb6",
            "ae9909fe1f9f44dda449d74298bcbc03",
            "de7e7ebd80074e6fa50cf1b169001eca",
            "c0e3cffa6ef84c4893dd179edcacc1b5",
            "a33390b2cf154b739e5d93bbbd10fa66",
            "9295a6a964554e598f3bfa443aa78002"
          ]
        },
        "outputId": "ec8e1e14-8c83-4084-a2c9-536e502b44d8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a7e37f321fb245e798aa3153f43f31ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d5f91a236ffe4b5fa89779b4c04e3cb6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae9909fe1f9f44dda449d74298bcbc03"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de7e7ebd80074e6fa50cf1b169001eca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c0e3cffa6ef84c4893dd179edcacc1b5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a33390b2cf154b739e5d93bbbd10fa66"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9295a6a964554e598f3bfa443aa78002"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Create tokenizer for setting up data in a format corresponding to the Mistral 7B model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "tokenizer.padding_side = 'right'\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.add_eos_token = True\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-20T12:51:40.532210Z",
          "iopub.execute_input": "2024-11-20T12:51:40.532474Z",
          "iopub.status.idle": "2024-11-20T12:51:41.736647Z",
          "shell.execute_reply.started": "2024-11-20T12:51:40.532449Z",
          "shell.execute_reply": "2024-11-20T12:51:41.735959Z"
        },
        "colab": {
          "referenced_widgets": [
            "232b55ab977d48eaaa5d31bc2eab3c42",
            "3ec78551f48b4700abd208d8cea56652",
            "ec1e43218d3b4defb126b971b46b749b",
            "0b5415a673ea4cda8a0e059b9dfe67c7"
          ]
        },
        "id": "9JhChxVidzgR",
        "outputId": "59cb923e-0438-484e-da3e-74552a967808"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/996 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "232b55ab977d48eaaa5d31bc2eab3c42"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3ec78551f48b4700abd208d8cea56652"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ec1e43218d3b4defb126b971b46b749b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0b5415a673ea4cda8a0e059b9dfe67c7"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training arguments\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=\"/kaggle/working/\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=1,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    save_steps=50,\n",
        "    logging_steps=50,\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.001,\n",
        "    fp16=False,\n",
        "    bf16=False,\n",
        "    max_grad_norm=0.3,\n",
        "    max_steps=-1,\n",
        "    warmup_ratio=0.03,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=\"constant\",\n",
        "    report_to=\"wandb\"\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-20T12:51:41.737745Z",
          "iopub.execute_input": "2024-11-20T12:51:41.738023Z",
          "iopub.status.idle": "2024-11-20T12:51:41.772525Z",
          "shell.execute_reply.started": "2024-11-20T12:51:41.737996Z",
          "shell.execute_reply": "2024-11-20T12:51:41.771759Z"
        },
        "id": "MY18SrVZdzgR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Define trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    max_seq_length= None,\n",
        "    dataset_text_field=\"text\",\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing= False,\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-20T12:51:41.773572Z",
          "iopub.execute_input": "2024-11-20T12:51:41.773849Z",
          "iopub.status.idle": "2024-11-20T12:51:43.122835Z",
          "shell.execute_reply.started": "2024-11-20T12:51:41.773822Z",
          "shell.execute_reply": "2024-11-20T12:51:43.121927Z"
        },
        "colab": {
          "referenced_widgets": [
            "76288c564c1744889d8219bc968d9ad7"
          ]
        },
        "id": "vx28UeptdzgS",
        "outputId": "13da8124-c2f0-4bef-9a7f-431ceb229bca"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field. Will not be supported from version '0.13.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:309: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/1853 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "76288c564c1744889d8219bc968d9ad7"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train and save the model:"
      ],
      "metadata": {
        "id": "MVMsvQCKdzgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-20T12:51:43.124060Z",
          "iopub.execute_input": "2024-11-20T12:51:43.124392Z",
          "iopub.status.idle": "2024-11-20T14:50:08.010605Z",
          "shell.execute_reply.started": "2024-11-20T12:51:43.124355Z",
          "shell.execute_reply": "2024-11-20T14:50:08.009798Z"
        },
        "id": "Ikb-ZR0zdzgS",
        "outputId": "58b314c2-86a7-433f-a3a9-4b2173b1aff8"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='464' max='464' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [464/464 1:57:43, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>2.054700</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>2.016700</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.976800</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>2.015500</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.925100</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.915300</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>1.915600</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.920900</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>1.905900</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
          "output_type": "stream"
        },
        {
          "execution_count": 16,
          "output_type": "execute_result",
          "data": {
            "text/plain": "TrainOutput(global_step=464, training_loss=1.9614676278212975, metrics={'train_runtime': 7104.1242, 'train_samples_per_second': 0.261, 'train_steps_per_second': 0.065, 'total_flos': 2.1608804352e+16, 'train_loss': 1.9614676278212975, 'epoch': 1.0})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "new_model = \"mistral_7b_lotte_michels\"\n",
        "trainer.model.save_pretrained(new_model)\n",
        "wandb.finish()\n",
        "model.config.use_cache = True\n",
        "trainer.model.push_to_hub(new_model, use_temp_dir=False)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-20T14:50:26.364237Z",
          "iopub.execute_input": "2024-11-20T14:50:26.364523Z",
          "iopub.status.idle": "2024-11-20T14:50:30.780198Z",
          "shell.execute_reply.started": "2024-11-20T14:50:26.364495Z",
          "shell.execute_reply": "2024-11-20T14:50:30.779415Z"
        },
        "colab": {
          "referenced_widgets": [
            "33ba1810d9274d09a0c8f0e55b4b503b"
          ]
        },
        "id": "qfAETYygdzgS",
        "outputId": "9f50a289-b7b7-45fd-ebd7-7316a1fafc94"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "33ba1810d9274d09a0c8f0e55b4b503b"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "No files have been modified since last commit. Skipping to prevent empty commit.\n",
          "output_type": "stream"
        },
        {
          "execution_count": 18,
          "output_type": "execute_result",
          "data": {
            "text/plain": "CommitInfo(commit_url='https://huggingface.co/lomi5/mistral_7b_lotte_michels/commit/e624fcddbc1d3b5d3679041d15f234149518d4bd', commit_message='Upload model', commit_description='', oid='e624fcddbc1d3b5d3679041d15f234149518d4bd', pr_url=None, repo_url=RepoUrl('https://huggingface.co/lomi5/mistral_7b_lotte_michels', endpoint='https://huggingface.co', repo_type='model', repo_id='lomi5/mistral_7b_lotte_michels'), pr_revision=None, pr_num=None)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "aygWY58ddzgS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Access the trained model and tokenizer and create an evaluation pipeline:"
      ],
      "metadata": {
        "id": "nvL_RPvudzgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from kaggle_secrets import UserSecretsClient\n",
        "user_secrets = UserSecretsClient()\n",
        "\n",
        "secret_hf = user_secrets.get_secret(\"huggingface\") # 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 REMOVE\n",
        "!huggingface-cli login --token $secret_hf\n",
        "\n",
        "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
        "model_adapter = \"lomi5/mistral_7b_lotte_michels\"\n",
        "base_model = AutoModelForCausalLM.from_pretrained(base_model_id)\n",
        "adapter_model = PeftModel.from_pretrained(base_model, model_adapter)\n",
        "\n",
        "print('model loaded')\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T09:28:13.321508Z",
          "iopub.execute_input": "2024-11-22T09:28:13.321866Z",
          "iopub.status.idle": "2024-11-22T09:30:36.019215Z",
          "shell.execute_reply.started": "2024-11-22T09:28:13.321830Z",
          "shell.execute_reply": "2024-11-22T09:30:36.017458Z"
        },
        "colab": {
          "referenced_widgets": [
            "143572784d37452fb899de3a55bb96f5",
            "0c0b48fd499f4447a6c9ef622d6d3210",
            "c58d8bb50b2e4bb5835b52f423f29bff",
            "3e38441b27514c9e8d5861f7ac14457b",
            "c2926f905126444c8b9a483df009d3d2",
            "3a43d5d69e0947dba87911588b9d5658",
            "46265055f82b4bfc85766d505aa57bd9",
            "eb00a79c8a2b4e56bec4850b467a1adb",
            "a4e446bcacf24e299dcf49a03f326621"
          ]
        },
        "id": "wUXCznR2dzgT",
        "outputId": "87ab74d8-9c62-4b34-dd85-5398edfb4747"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nThe token `meddaiproject` has been saved to /root/.cache/huggingface/stored_tokens\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful.\nThe current active token is: `meddaiproject`\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "143572784d37452fb899de3a55bb96f5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c0b48fd499f4447a6c9ef622d6d3210"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c58d8bb50b2e4bb5835b52f423f29bff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e38441b27514c9e8d5861f7ac14457b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c2926f905126444c8b9a483df009d3d2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3a43d5d69e0947dba87911588b9d5658"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "46265055f82b4bfc85766d505aa57bd9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "adapter_config.json:   0%|          | 0.00/695 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb00a79c8a2b4e56bec4850b467a1adb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "adapter_model.safetensors:   0%|          | 0.00/369M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a4e446bcacf24e299dcf49a03f326621"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "model loaded\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_id = \"mistralai/Mistral-7B-v0.1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T09:30:39.342448Z",
          "iopub.execute_input": "2024-11-22T09:30:39.342841Z",
          "iopub.status.idle": "2024-11-22T09:30:40.552194Z",
          "shell.execute_reply.started": "2024-11-22T09:30:39.342808Z",
          "shell.execute_reply": "2024-11-22T09:30:40.551249Z"
        },
        "colab": {
          "referenced_widgets": [
            "842dce909dd04682a2cc9efe91de395b",
            "918258d83db54b6da2e347bd42f985a5",
            "8a05b0a15c284f679c22b796065c4f81",
            "8545dda57bb04e38b8bf908767045cff"
          ]
        },
        "id": "Z2yiKaUbdzgT",
        "outputId": "14b4faf8-2a50-4557-b24f-54c17d7a9bf0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/996 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "842dce909dd04682a2cc9efe91de395b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "918258d83db54b6da2e347bd42f985a5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a05b0a15c284f679c22b796065c4f81"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8545dda57bb04e38b8bf908767045cff"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "logging.set_verbosity(logging.CRITICAL)\n",
        "pipe = pipeline(task=\"text-generation\", model=adapter_model, tokenizer=tokenizer, max_length=300)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T09:30:45.489069Z",
          "iopub.execute_input": "2024-11-22T09:30:45.489848Z",
          "iopub.status.idle": "2024-11-22T09:30:45.495253Z",
          "shell.execute_reply.started": "2024-11-22T09:30:45.489803Z",
          "shell.execute_reply": "2024-11-22T09:30:45.494327Z"
        },
        "id": "8hR9uprsdzgT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test two user scenarios:"
      ],
      "metadata": {
        "id": "SPxn6v1ydzgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# User Scenario 1: Giving advice on medication use/(side)effects\n",
        "\n",
        "prompt1 = \"my dad recently has a surgery for head injury with subdural haematoma after that he has been prescribed eptoin which is an anti-epileptic drug. my dad had no history of epilepsy . he has been complaining of headache ever since which i now understand is one of many side effects of this drug. why does he need to take this drug? does he need to continue taking it and if so for how long? thanks anand\"\n",
        "answer1 = \"hello awasthi [SEP] welcome to health care magic [SEP] after brain surgery for head injury due to sub dural hematoma, eptoin or any anti epileptic drug is given to patient for prophylaxis of any seizure episode that may occur following surgery. it is a routine practice. so even if there is no history of seizure or epilepsy, anti epileptic drugs are given. regarding headache, although with phenatoin headache may occur as side effect but it non significant, if your father is experiencing headache it may be due to trauma itself, or there may be raised intracranial pressure which may cause headache. eptoin has nothing to do with headache, and yes he has to take this medicine for at least some months, and there is no harm. [SEP] thanks [SEP] take care\"\n",
        "result1 = pipe(f\"<|startofquestion|> {prompt1} <|endofquestion|>\")\n",
        "print(result1[0]['generated_text'], \"\\n\")\n",
        "print(\"Expert provided answer: \\n\", answer1, \"\\n\")\n",
        "\n",
        "prompt2 = \"hi doc my baby is 5 month old and my doc suggested coscopin 3ml 3 times a day. is it ok to give so high dosage to small baby. i was hesitant so initially i gave him 1.5 ml then 2.5 ml and 3ml and 1.5 ml. total 4 dosage. doc told he has dry cough . i am worried. he has cough from last 1 month. we thought it will be cured by its own. but now me worried for side effects of this medicine\"\n",
        "answer2 = \"hi thanks for the query to hcm .sorry to hear that your baby is having dry cough .coscopin is prescribed ( noscapine 1.83 mg, sodium citrate 0.67 mgwhile ammonium chloride 7.0mg).this is a normal dose for a baby of 5 month , so don't worry about the side effect of these ingredient as all are safe and dosage are according to age .as you stated that your 5 month baby is having dry cough for 1 month.in my opinion , consult one more paed doctor for 2nd opinion for dry cough because cough is for 1 month.hope this information will be useful for you .good luck for baby.\"\n",
        "result2 = pipe(f\"<|startofquestion|> {prompt2} <|endofquestion|>\")\n",
        "print(result2[0]['generated_text'], \"\\n\")\n",
        "print(\"Expert provided answer: \\n\", answer2)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T09:30:53.546393Z",
          "iopub.execute_input": "2024-11-22T09:30:53.546961Z",
          "iopub.status.idle": "2024-11-22T09:38:51.230032Z",
          "shell.execute_reply.started": "2024-11-22T09:30:53.546929Z",
          "shell.execute_reply": "2024-11-22T09:38:51.229077Z"
        },
        "id": "cI_jO_bcdzgT",
        "outputId": "85da9388-7c26-4bee-cc80-d8a1ece5b9b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "<|startofquestion|> my dad recently has a surgery for head injury with subdural haematoma after that he has been prescribed eptoin which is an anti-epileptic drug. my dad had no history of epilepsy . he has been complaining of headache ever since which i now understand is one of many side effects of this drug. why does he need to take this drug? does he need to continue taking it and if so for how long? thanks anand <|endofquestion|> <|startofanswer|> hi, [SEP] welcome to healthcare magic. [SEP] eptoin is an anti-epileptic drug. it is used to prevent seizures. it is also used to treat migraine headaches. [SEP] if your dad has no history of epilepsy, then he should not be taking eptoin. [SEP] if he has a history of seizures, then he should continue taking eptoin. [SEP] if he has a history of migraine headaches, then he should continue taking eptoin. [SEP] if he has no history of seizures or migraine headaches, then he should stop taking eptoin. [SEP] hope i have answered your query. let me know if i can assist you further. [SEP] regards, [SEP] dr. shinas hussain, \n\nExpert provided answer: \n hello awasthi [SEP] welcome to health care magic [SEP] after brain surgery for head injury due to sub dural hematoma, eptoin or any anti epileptic drug is given to patient for prophylaxis of any seizure episode that may occur following surgery. it is a routine practice. so even if there is no history of seizure or epilepsy, anti epileptic drugs are given. regarding headache, although with phenatoin headache may occur as side effect but it non significant, if your father is experiencing headache it may be due to trauma itself, or there may be raised intracranial pressure which may cause headache. eptoin has nothing to do with headache, and yes he has to take this medicine for at least some months, and there is no harm. [SEP] thanks [SEP] take care \n\n<|startofquestion|> hi doc my baby is 5 month old and my doc suggested coscopin 3ml 3 times a day. is it ok to give so high dosage to small baby. i was hesitant so initially i gave him 1.5 ml then 2.5 ml and 3ml and 1.5 ml. total 4 dosage. doc told he has dry cough . i am worried. he has cough from last 1 month. we thought it will be cured by its own. but now me worried for side effects of this medicine <|endofquestion|> <|startofanswer|> hi, [SEP] welcome to healthcare magic. i understand your concern. [SEP] the dose of coscopin is 3 ml three times a day. it is not a high dose. it is a safe dose. [SEP] the side effects of this medicine are very less. [SEP] hope i have answered your query. let me know if i can assist you further. [SEP] regards, [SEP] dr. shinas hussain, pediatrician <|endofanswer|> <|startofquestion|> hi doctor, [SEP] i am 22 years old. i have a problem with my vagina. i have a small hole in my vagina. i am not able to have sex. \n\nExpert provided answer: \n hi thanks for the query to hcm .sorry to hear that your baby is having dry cough .coscopin is prescribed ( noscapine 1.83 mg, sodium citrate 0.67 mgwhile ammonium chloride 7.0mg).this is a normal dose for a baby of 5 month , so don't worry about the side effect of these ingredient as all are safe and dosage are according to age .as you stated that your 5 month baby is having dry cough for 1 month.in my opinion , consult one more paed doctor for 2nd opinion for dry cough because cough is for 1 month.hope this information will be useful for you .good luck for baby.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# User Scenario 2: Does the model advice to seek a real doctor's counsel for complex medical complaints?\n",
        "\n",
        "prompt3 = \"hi, my sister in law who is aged 36 years is suffering from bipolar disorder. she is under following medication from last 1.5 years: clonotril 1 mg, tolaz 5 mg, parkin 4 mg, nexito 5 mg all the tablets consumed each every day. at times her face becomes swollen and during that time, she says that is afraid that she is going to die and she loses her self control. her bp shoots up high. kindly advise if the medications are under the right directions. should there be increase or decrease/change of medications? regards, thamayanthi r\"\n",
        "answer3 = \"hi, [SEP] it is not in your sister in law's best interest for physicians on this network to determine the validity of any treatment which has already been advised by another physician from whom she currently receives treatment. [SEP] it is, however, within the bounds of our knowledge base to tell you that it would be perfectly reasonable for you to encourage your sister-in-law to be honest with her treating physician and relate the side effects you mention (now or at the next visit) so that the physician involved can determine the validity of the medication regimen, the dosages, and the possibility of substitutions or other changes for her to feel better. this is the best course of action your sister-in-law could take for herself. [SEP] hope i have answered your query. let me know if i can assist you further. [SEP] regards, [SEP] dr. dariush saghafi, [SEP] neurologist\"\n",
        "result3 = pipe(f\"<|startofquestion|> {prompt3} <|endofquestion|>\")\n",
        "print(result3[0]['generated_text'], \"\\n\")\n",
        "print(\"Expert provided answer: \\n\", answer3, '\\n')\n",
        "\n",
        "prompt4 = \"hello. i am a 26yr old female. i have some sort of condition that no-one seems to be able to diagnose. i have had this for 7 years (since the birth of my child) and it has been getting progressively worse to the point where i can t leave the house. symptoms include - palpatations pvc s/pac s gerd shortness of breath (as if my lungs are spring loaded) hot weak feeling in chest, sometimes with a buzzing sensation severe panic attacks - sometimes random but mostly when leaving the house or having to be around people. dizziness and fainting stomach pains (tearing, aching, cramping) muscle spasms (all over) back pain constant nasal drip trapped nerves in back, shoulders, and ribs costochondritis general widespread pain alcohol helps a lot with pain management and panic attacks which could point to a psychosomatic illness? thank you for your help.\"\n",
        "answer4 = \"hi, [SEP] you should see a psychiatrist for proper diagnosis of your phobia/panic attack. [SEP] you should also practice relaxation techniques like deep breathing exercises. [SEP] hope i have answered your query. let me know if i can assist you further. [SEP] regards, [SEP] dr. varinder joshi, [SEP] general & family physician\"\n",
        "result4 = pipe(f\"<|startofquestion|> {prompt4} <|endofquestion|>\")\n",
        "print(result4[0]['generated_text'], \"\\n\")\n",
        "print(\"Expert provided answer: \\n\", answer4)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-22T09:38:51.231369Z",
          "iopub.execute_input": "2024-11-22T09:38:51.231628Z",
          "iopub.status.idle": "2024-11-22T09:44:05.119855Z",
          "shell.execute_reply.started": "2024-11-22T09:38:51.231602Z",
          "shell.execute_reply": "2024-11-22T09:44:05.118792Z"
        },
        "id": "XoO0bFeJdzgU",
        "outputId": "491f360a-1aad-45f9-b71c-821cc8ba85e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "<|startofquestion|> hi, my sister in law who is aged 36 years is suffering from bipolar disorder. she is under following medication from last 1.5 years: clonotril 1 mg, tolaz 5 mg, parkin 4 mg, nexito 5 mg all the tablets consumed each every day. at times her face becomes swollen and during that time, she says that is afraid that she is going to die and she loses her self control. her bp shoots up high. kindly advise if the medications are under the right directions. should there be increase or decrease/change of medications? regards, thamayanthi r <|endofquestion|> <|startofanswer|> hi, [SEP] welcome to healthcare magic. i can understand your concern. bipolar disorder is a chronic illness and requires lifelong treatment. the medications you have mentioned are good for bipolar disorder. the swelling of face and fear of dying are the symptoms of mania. the medications you have mentioned are good for mania. the medications should be continued for a long time. the dose of medications should be increased or decreased only after consulting your psychiatrist. [SEP] hope i have answered your query. let me know if i can assist you further. [SEP] regards, [SEP] dr. sumanth mbbs., \n\nExpert provided answer: \n hi, [SEP] it is not in your sister in law's best interest for physicians on this network to determine the validity of any treatment which has already been advised by another physician from whom she currently receives treatment. [SEP] it is, however, within the bounds of our knowledge base to tell you that it would be perfectly reasonable for you to encourage your sister-in-law to be honest with her treating physician and relate the side effects you mention (now or at the next visit) so that the physician involved can determine the validity of the medication regimen, the dosages, and the possibility of substitutions or other changes for her to feel better. this is the best course of action your sister-in-law could take for herself. [SEP] hope i have answered your query. let me know if i can assist you further. [SEP] regards, [SEP] dr. dariush saghafi, [SEP] neurologist \n\n<|startofquestion|> hello. i am a 26yr old female. i have some sort of condition that no-one seems to be able to diagnose. i have had this for 7 years (since the birth of my child) and it has been getting progressively worse to the point where i can t leave the house. symptoms include - palpatations pvc s/pac s gerd shortness of breath (as if my lungs are spring loaded) hot weak feeling in chest, sometimes with a buzzing sensation severe panic attacks - sometimes random but mostly when leaving the house or having to be around people. dizziness and fainting stomach pains (tearing, aching, cramping) muscle spasms (all over) back pain constant nasal drip trapped nerves in back, shoulders, and ribs costochondritis general widespread pain alcohol helps a lot with pain management and panic attacks which could point to a psychosomatic illness? thank you for your help. <|endofquestion|> <|startofanswer|> hello, [SEP] welcome to healthcare magic. i can understand your concern. you may have panic disorder. you should consult a psychiatrist and get evaluated. you may require psychotherapy and antidepressant medication. you should avoid alcohol. you should do regular exercise and yoga. hope i have answered your query. let me know if i can \n\nExpert provided answer: \n hi, [SEP] you should see a psychiatrist for proper diagnosis of your phobia/panic attack. [SEP] you should also practice relaxation techniques like deep breathing exercises. [SEP] hope i have answered your query. let me know if i can assist you further. [SEP] regards, [SEP] dr. varinder joshi, [SEP] general & family physician\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}
